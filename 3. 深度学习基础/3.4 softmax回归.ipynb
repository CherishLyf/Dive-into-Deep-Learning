{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd079348f91ea695bd81548cae96f1c95c65b4495c24fade6630ca4ab5031074804",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 3.4 softmax 回归\n",
    "\n",
    "和线性回归不同，softmax回归的输出单元从一个变成了多个，且引入了softmax运算使输出更适合离散值的预测和训练。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.4.1 分类问题\n",
    "\n",
    "离散值输出的模型来解决分类问题。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.4.2 softmax 分类模型\n",
    "\n",
    "softmax回归跟线性回归一样将输入特征与权重做线性叠加。与线性回归的一个主要不同在于，softmax回归的输出值个数等于标签里的类别数。比如，目前有4个特征和3种标签,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "o_1 &= x_1 w_{11} + x_2 w_{21} + x_3 w_{31} + x_4 w_{41} + b_1,\\\\\n",
    "o_2 &= x_1 w_{12} + x_2 w_{22} + x_3 w_{32} + x_4 w_{42} + b_2,\\\\\n",
    "o_3 &= x_1 w_{13} + x_2 w_{23} + x_3 w_{33} + x_4 w_{43} + b_3.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "softmax回归同线性回归一样，也是一个单层神经网络。由于每个输出$o_1, o_2, o_3$的计算都要依赖于所有的输入$x_1, x_2, x_3, x_4$，softmax回归的输出层也是一个全连接层。\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"350\" src=\"http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter03/3.4_softmaxreg.svg\"/>\n",
    "</div>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "softmax运算符（softmax operator）解决了以上两个问题。它通过下式将输出值变换成值为正且和为1的概率分布：\n",
    "\n",
    "$$\n",
    "\\hat{y}_1, \\hat{y}_2, \\hat{y}_3 = \\text{softmax}(o_1, o_2, o_3)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y}_1 = \\frac{ \\exp(o_1)}{\\sum_{i=1}^3 \\exp(o_i)},\\quad\n",
    "\\hat{y}_2 = \\frac{ \\exp(o_2)}{\\sum_{i=1}^3 \\exp(o_i)},\\quad\n",
    "\\hat{y}_3 = \\frac{ \\exp(o_3)}{\\sum_{i=1}^3 \\exp(o_i)}.\n",
    "$$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.4.3 单样本分类的矢量计算表达式\n",
    "\n",
    "为了提高计算效率，我们可以将单样本分类通过矢量计算来表达。在上面的图像分类问题中，假设softmax回归的权重和偏差参数分别为\n",
    "\n",
    "$$\n",
    "\\boldsymbol{W} = \n",
    "\\begin{bmatrix}\n",
    "    w_{11} & w_{12} & w_{13} \\\\\n",
    "    w_{21} & w_{22} & w_{23} \\\\\n",
    "    w_{31} & w_{32} & w_{33} \\\\\n",
    "    w_{41} & w_{42} & w_{43}\n",
    "\\end{bmatrix},\\quad\n",
    "\\boldsymbol{b} = \n",
    "\\begin{bmatrix}\n",
    "    b_1 & b_2 & b_3\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "设高和宽分别为2个像素的图像样本$i$的特征为\n",
    "\n",
    "$$\\boldsymbol{x}^{(i)} = \\begin{bmatrix}x_1^{(i)} & x_2^{(i)} & x_3^{(i)} & x_4^{(i)}\\end{bmatrix},$$\n",
    "\n",
    "输出层的输出为\n",
    "\n",
    "$$\\boldsymbol{o}^{(i)} = \\begin{bmatrix}o_1^{(i)} & o_2^{(i)} & o_3^{(i)}\\end{bmatrix},$$\n",
    "\n",
    "预测为狗、猫或鸡的概率分布为\n",
    "\n",
    "$$\\boldsymbol{\\hat{y}}^{(i)} = \\begin{bmatrix}\\hat{y}_1^{(i)} & \\hat{y}_2^{(i)} & \\hat{y}_3^{(i)}\\end{bmatrix}.$$\n",
    "\n",
    "\n",
    "softmax回归对样本$i$分类的矢量计算表达式为\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{o}^{(i)} &= \\boldsymbol{x}^{(i)} \\boldsymbol{W} + \\boldsymbol{b},\\\\\n",
    "\\boldsymbol{\\hat{y}}^{(i)} &= \\text{softmax}(\\boldsymbol{o}^{(i)}).\n",
    "\\end{aligned}\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "## 3.4.4 小批量样本分类的矢量计算表达式\n",
    "\n",
    "\n",
    "为了进一步提升计算效率，我们通常对小批量数据做矢量计算。广义上讲，给定一个小批量样本，其批量大小为$n$，输入个数（特征数）为$d$，输出个数（类别数）为$q$。设批量特征为$\\boldsymbol{X} \\in \\mathbb{R}^{n \\times d}$。假设softmax回归的权重和偏差参数分别为$\\boldsymbol{W} \\in \\mathbb{R}^{d \\times q}$和$\\boldsymbol{b} \\in \\mathbb{R}^{1 \\times q}$。softmax回归的矢量计算表达式为\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{O} &= \\boldsymbol{X} \\boldsymbol{W} + \\boldsymbol{b},\\\\\n",
    "\\boldsymbol{\\hat{Y}} &= \\text{softmax}(\\boldsymbol{O}),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中的加法运算使用了广播机制，$\\boldsymbol{O}, \\boldsymbol{\\hat{Y}} \\in \\mathbb{R}^{n \\times q}$且这两个矩阵的第$i$行分别为样本$i$的输出$\\boldsymbol{o}^{(i)}$和概率分布$\\boldsymbol{\\hat{y}}^{(i)}$。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.4.5 交叉熵函数\n",
    "\n",
    "交叉熵(cross entropy)是一个常用的衡量两个概率分布的测量函数。\n",
    "\n",
    "$$H\\left(\\boldsymbol y^{(i)}, \\boldsymbol {\\hat y}^{(i)}\\right ) = -\\sum_{j=1}^q y_j^{(i)} \\log \\hat y_j^{(i)},$$\n",
    "\n",
    "假设数据集的样本个数为n，那么交叉熵的损失函数为：\n",
    "\n",
    "$$\\ell(\\boldsymbol{\\Theta}) = \\frac{1}{n} \\sum_{i=1}^n H\\left(\\boldsymbol y^{(i)}, \\boldsymbol {\\hat y}^{(i)}\\right ),$$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.4.6 模型预测及评价\n",
    "\n",
    "我们将使用准确率（accuracy）来评价模型的表现。它等于正确预测数量与总预测数量之比。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 小结\n",
    "\n",
    "* softmax回归适用于分类问题。它使用softmax运算输出类别的概率分布。\n",
    "* softmax回归是一个单层神经网络，输出个数等于分类问题中的类别个数。\n",
    "* 交叉熵适合衡量两个概率分布的差异。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}